# Нейронный машинный перевод

Цель этого задания обучить систему машинного перевода на основе реккуретных сетей с механизмом attention.
Этот кейс также сделан в форме проекта с использованием фреймворка Pytorch Lightning.

## Установка
Чтобы нам было удобнее работать и запускать код, надо первым делом установить пакет в 
режиме разработчика
```bash
pip install -e .
```

`pip install` -- установка пакета в нашу среду.

# Данные

Взят корпус параллельных текстов OpenSubtitles. Процесс загрузки датасета описан в ```get_data.py```

# Запуск

## Установка зависимостей

```bash
pip install -r requirements.txt
```

## Скрипт ```run.py```, аргументы:
- directory — папка с данными, включая токенизаторы  
- checkpoint_path — путь для сохранения чекпоинта модели  
- project_name — название проекта  
- verbose — дополнительный вывод в консоль некоторых действий  
- load_data — флаг, при указании которого загружаются и подготавливаются данные  
- train_tokenizers — флаг, при указании которого обучаются токенизаторы  
- max_norm — максимальная норма при клиппинге градиентов  
- epochs — количество эпох обучения  
- batch_size — размер батча  
- max_length — максимальная длина последовательности  
- n_batch_accumulate — количество батчей для аккамуляции градиентов  
- seed — число для фиксирования рандомных параметров  
- train_n_pairs — количество пар текстов для обучения  
- valid_n_pairs — количество пар текстов для валидации  
- pad_index — индекс токена PAD  
- bos_index — индекс токена BOS  
- eos_index — индекс токена EOS  
- vocab_size — размер словаря  
- embedding_dim — размерность эмбеддингов  
- model_dim — размерность модели  
- encoder_num_layers — количество слоев RNN в энкодере  
- decoder_num_layers — количество слоев RNN в декодере  
- dropout — вероятность дропаута  
- attention_dropout — вероятность дропаута для аттеншина  
- weight_tying — флаг, при указании которого шарятся веса эмбеддингов целевого языка
и выходной матрицы для предсказания слов
- bidirectional_encoder — флаг, при указании которого декодер будет двунаправленным
- use_attention — флаг, при указании которого используется аттеншин в декодере  
- learning_rate — величина learning rate для оптимизатора  
- weight_decay — величина weight decay для оптимизатора

## Первый запуск
При первом запуске нужно указать флаги load_data и train_tokenizers, чтобы загрузить данные
и обучить токенизаторы. В последующих запусках эти этапы можно пропустить. В остальные запуски дефолтные значения уже установлены
и можно начать с них.

# Задания

## ```src/model```
Реализовать методы: ```forward``` и ```generate```

### ```forward```
Этот метод должен описать всю работу sequence2sequence модели, включая использование энкодера, сохранение его памяти и передачу этой памяти в декодер.
Также работу с самим декодером, то есть обработку ```target``` последовательности, используя память от энкодера. В этом задании также добавляется механизм attention,
 который нужно реализовать самоятостельно. Весь остальной код вы можете переиспользовать из предыдущего задания.

### ```generate```
Этот метод должен описывать жадную генерацию перевода. На вход принимаются тензор, состоящий из индексов слов на английском языке
и на выход отдается список списков, которые состоят из индексов слов на целевом (русском) языке. Не забудьте в начало генерации добавить в декодер тег BOS, который говорит о начале генерации.
Индекс тега BOS доступен по ссылке ```self.bos_index```
Стоит отметить, что в этом задании из-за изменения метода ```forward``` меняется и генерация текстов, поэтому вам нужно учитывать в этом методе то, как вы реализовали attention в предыдущем пункте.
Остальной код также можно переиспользовать из предыдущего задания.

## Запуск тестов
Необходимое, но недостаточное условие сдачи задания -- прохождение всех тестов.
Тесты находятся в папке tests, чтобы их запустить, в корне репозитория введите
```bash
pytest
```

## Сдача задания

Загрузите ваше решение на GitHub и отправьте ссылку на репозиторий и на ваш Wandb проект.
